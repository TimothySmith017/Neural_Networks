{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n0   41       Yes      Travel_Rarely       1102                   Sales   \n1   49        No  Travel_Frequently        279  Research & Development   \n2   37       Yes      Travel_Rarely       1373  Research & Development   \n3   33        No  Travel_Frequently       1392  Research & Development   \n4   27        No      Travel_Rarely        591  Research & Development   \n\n   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n0                 1          2  Life Sciences              1               1   \n1                 8          1  Life Sciences              1               2   \n2                 2          2          Other              1               4   \n3                 3          4  Life Sciences              1               5   \n4                 2          1        Medical              1               7   \n\n   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n0  ...                         1            80                 0   \n1  ...                         4            80                 1   \n2  ...                         2            80                 0   \n3  ...                         3            80                 0   \n4  ...                         4            80                 1   \n\n   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n0                  8                      0               1               6   \n1                 10                      3               3              10   \n2                  7                      3               3               0   \n3                  8                      3               3               8   \n4                  6                      3               3               2   \n\n  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n0                  4                        0                     5  \n1                  7                        1                     7  \n2                  0                        0                     0  \n3                  7                        3                     0  \n4                  2                        2                     2  \n\n[5 rows x 35 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Attrition</th>\n      <th>BusinessTravel</th>\n      <th>DailyRate</th>\n      <th>Department</th>\n      <th>DistanceFromHome</th>\n      <th>Education</th>\n      <th>EducationField</th>\n      <th>EmployeeCount</th>\n      <th>EmployeeNumber</th>\n      <th>...</th>\n      <th>RelationshipSatisfaction</th>\n      <th>StandardHours</th>\n      <th>StockOptionLevel</th>\n      <th>TotalWorkingYears</th>\n      <th>TrainingTimesLastYear</th>\n      <th>WorkLifeBalance</th>\n      <th>YearsAtCompany</th>\n      <th>YearsInCurrentRole</th>\n      <th>YearsSinceLastPromotion</th>\n      <th>YearsWithCurrManager</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>Yes</td>\n      <td>Travel_Rarely</td>\n      <td>1102</td>\n      <td>Sales</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Life Sciences</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>80</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>No</td>\n      <td>Travel_Frequently</td>\n      <td>279</td>\n      <td>Research &amp; Development</td>\n      <td>8</td>\n      <td>1</td>\n      <td>Life Sciences</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>4</td>\n      <td>80</td>\n      <td>1</td>\n      <td>10</td>\n      <td>3</td>\n      <td>3</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>Yes</td>\n      <td>Travel_Rarely</td>\n      <td>1373</td>\n      <td>Research &amp; Development</td>\n      <td>2</td>\n      <td>2</td>\n      <td>Other</td>\n      <td>1</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2</td>\n      <td>80</td>\n      <td>0</td>\n      <td>7</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33</td>\n      <td>No</td>\n      <td>Travel_Frequently</td>\n      <td>1392</td>\n      <td>Research &amp; Development</td>\n      <td>3</td>\n      <td>4</td>\n      <td>Life Sciences</td>\n      <td>1</td>\n      <td>5</td>\n      <td>...</td>\n      <td>3</td>\n      <td>80</td>\n      <td>0</td>\n      <td>8</td>\n      <td>3</td>\n      <td>3</td>\n      <td>8</td>\n      <td>7</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>27</td>\n      <td>No</td>\n      <td>Travel_Rarely</td>\n      <td>591</td>\n      <td>Research &amp; Development</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Medical</td>\n      <td>1</td>\n      <td>7</td>\n      <td>...</td>\n      <td>4</td>\n      <td>80</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 35 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import our input dataset\n",
    "attrition_df = pd.read_csv('HR-Employee-Attrition.csv')\n",
    "attrition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our categorical variable list\n",
    "attrition_cat = attrition_df.dtypes[attrition_df.dtypes == \"object\"].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Attrition         2\nBusinessTravel    3\nDepartment        3\nEducationField    6\nGender            2\nJobRole           9\nMaritalStatus     3\nOver18            1\nOverTime          2\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# Check the number of unique values in each column\n",
    "attrition_df[attrition_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Attrition_No  Attrition_Yes  BusinessTravel_Non-Travel  \\\n0           0.0            1.0                        0.0   \n1           1.0            0.0                        0.0   \n2           0.0            1.0                        0.0   \n3           1.0            0.0                        0.0   \n4           1.0            0.0                        0.0   \n\n   BusinessTravel_Travel_Frequently  BusinessTravel_Travel_Rarely  \\\n0                               0.0                           1.0   \n1                               1.0                           0.0   \n2                               0.0                           1.0   \n3                               1.0                           0.0   \n4                               0.0                           1.0   \n\n   Department_Human Resources  Department_Research & Development  \\\n0                         0.0                                0.0   \n1                         0.0                                1.0   \n2                         0.0                                1.0   \n3                         0.0                                1.0   \n4                         0.0                                1.0   \n\n   Department_Sales  EducationField_Human Resources  \\\n0               1.0                             0.0   \n1               0.0                             0.0   \n2               0.0                             0.0   \n3               0.0                             0.0   \n4               0.0                             0.0   \n\n   EducationField_Life Sciences  ...  JobRole_Research Director  \\\n0                           1.0  ...                        0.0   \n1                           1.0  ...                        0.0   \n2                           0.0  ...                        0.0   \n3                           1.0  ...                        0.0   \n4                           0.0  ...                        0.0   \n\n   JobRole_Research Scientist  JobRole_Sales Executive  \\\n0                         0.0                      1.0   \n1                         1.0                      0.0   \n2                         0.0                      0.0   \n3                         1.0                      0.0   \n4                         0.0                      0.0   \n\n   JobRole_Sales Representative  MaritalStatus_Divorced  \\\n0                           0.0                     0.0   \n1                           0.0                     0.0   \n2                           0.0                     0.0   \n3                           0.0                     0.0   \n4                           0.0                     0.0   \n\n   MaritalStatus_Married  MaritalStatus_Single  Over18_Y  OverTime_No  \\\n0                    0.0                   1.0       1.0          0.0   \n1                    1.0                   0.0       1.0          1.0   \n2                    0.0                   1.0       1.0          0.0   \n3                    1.0                   0.0       1.0          0.0   \n4                    1.0                   0.0       1.0          1.0   \n\n   OverTime_Yes  \n0           1.0  \n1           0.0  \n2           1.0  \n3           1.0  \n4           0.0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Attrition_No</th>\n      <th>Attrition_Yes</th>\n      <th>BusinessTravel_Non-Travel</th>\n      <th>BusinessTravel_Travel_Frequently</th>\n      <th>BusinessTravel_Travel_Rarely</th>\n      <th>Department_Human Resources</th>\n      <th>Department_Research &amp; Development</th>\n      <th>Department_Sales</th>\n      <th>EducationField_Human Resources</th>\n      <th>EducationField_Life Sciences</th>\n      <th>...</th>\n      <th>JobRole_Research Director</th>\n      <th>JobRole_Research Scientist</th>\n      <th>JobRole_Sales Executive</th>\n      <th>JobRole_Sales Representative</th>\n      <th>MaritalStatus_Divorced</th>\n      <th>MaritalStatus_Married</th>\n      <th>MaritalStatus_Single</th>\n      <th>Over18_Y</th>\n      <th>OverTime_No</th>\n      <th>OverTime_Yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(attrition_df[attrition_cat]))\n",
    "\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encode_df.columns = enc.get_feature_names(attrition_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Age  DailyRate  DistanceFromHome  Education  EmployeeCount  EmployeeNumber  \\\n0   41       1102                 1          2              1               1   \n1   49        279                 8          1              1               2   \n2   37       1373                 2          2              1               4   \n3   33       1392                 3          4              1               5   \n4   27        591                 2          1              1               7   \n\n   EnvironmentSatisfaction  HourlyRate  JobInvolvement  JobLevel  ...  \\\n0                        2          94               3         2  ...   \n1                        3          61               2         2  ...   \n2                        4          92               2         1  ...   \n3                        4          56               3         1  ...   \n4                        1          40               3         1  ...   \n\n   JobRole_Research Director  JobRole_Research Scientist  \\\n0                        0.0                         0.0   \n1                        0.0                         1.0   \n2                        0.0                         0.0   \n3                        0.0                         1.0   \n4                        0.0                         0.0   \n\n   JobRole_Sales Executive  JobRole_Sales Representative  \\\n0                      1.0                           0.0   \n1                      0.0                           0.0   \n2                      0.0                           0.0   \n3                      0.0                           0.0   \n4                      0.0                           0.0   \n\n   MaritalStatus_Divorced  MaritalStatus_Married  MaritalStatus_Single  \\\n0                     0.0                    0.0                   1.0   \n1                     0.0                    1.0                   0.0   \n2                     0.0                    0.0                   1.0   \n3                     0.0                    1.0                   0.0   \n4                     0.0                    1.0                   0.0   \n\n   Over18_Y  OverTime_No  OverTime_Yes  \n0       1.0          0.0           1.0  \n1       1.0          1.0           0.0  \n2       1.0          0.0           1.0  \n3       1.0          0.0           1.0  \n4       1.0          1.0           0.0  \n\n[5 rows x 57 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>DailyRate</th>\n      <th>DistanceFromHome</th>\n      <th>Education</th>\n      <th>EmployeeCount</th>\n      <th>EmployeeNumber</th>\n      <th>EnvironmentSatisfaction</th>\n      <th>HourlyRate</th>\n      <th>JobInvolvement</th>\n      <th>JobLevel</th>\n      <th>...</th>\n      <th>JobRole_Research Director</th>\n      <th>JobRole_Research Scientist</th>\n      <th>JobRole_Sales Executive</th>\n      <th>JobRole_Sales Representative</th>\n      <th>MaritalStatus_Divorced</th>\n      <th>MaritalStatus_Married</th>\n      <th>MaritalStatus_Single</th>\n      <th>Over18_Y</th>\n      <th>OverTime_No</th>\n      <th>OverTime_Yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>1102</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>94</td>\n      <td>3</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>279</td>\n      <td>8</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>61</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>1373</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>92</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33</td>\n      <td>1392</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>56</td>\n      <td>3</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>27</td>\n      <td>591</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>1</td>\n      <td>40</td>\n      <td>3</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 57 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "attrition_df = attrition_df.merge(encode_df,left_index=True, right_index=True)\n",
    "attrition_df = attrition_df.drop(attrition_cat,1)\n",
    "attrition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y= attrition_df[\"Attrition_Yes\"].values\n",
    "X = attrition_df.drop([\"Attrition_Yes\",\"Attrition_No\"],1).values\n",
    "\n",
    "#Split the processed data into test/train group\n",
    "X_train, X_test, y_train, y_test, = train_test_split(X,y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_12 (Dense)             (None, 8)                 448       \n_________________________________________________________________\ndense_13 (Dense)             (None, 5)                 45        \n_________________________________________________________________\ndense_14 (Dense)             (None, 1)                 6         \n=================================================================\nTotal params: 499\nTrainable params: 499\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 =  8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "32/1102 [..............................] - ETA: 0s - loss: 0.2531 - accuracy: 0.9062\nEpoch 00025: saving model to checkpoints/weights.25.hdf5\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.3018 - accuracy: 0.8848\nEpoch 26/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3458 - accuracy: 0.8438\nEpoch 00026: saving model to checkpoints/weights.26.hdf5\n1102/1102 [==============================] - 0s 91us/sample - loss: 0.2991 - accuracy: 0.8929\nEpoch 27/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2677 - accuracy: 0.8438\nEpoch 00027: saving model to checkpoints/weights.27.hdf5\n1102/1102 [==============================] - 0s 90us/sample - loss: 0.2961 - accuracy: 0.8884\nEpoch 28/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1868 - accuracy: 0.9375\nEpoch 00028: saving model to checkpoints/weights.28.hdf5\n1102/1102 [==============================] - 0s 83us/sample - loss: 0.2935 - accuracy: 0.8893\nEpoch 29/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3425 - accuracy: 0.8125\nEpoch 00029: saving model to checkpoints/weights.29.hdf5\n1102/1102 [==============================] - 0s 90us/sample - loss: 0.2899 - accuracy: 0.8929\nEpoch 30/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2859 - accuracy: 0.9062\nEpoch 00030: saving model to checkpoints/weights.30.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2880 - accuracy: 0.8875\nEpoch 31/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.5227 - accuracy: 0.7812\nEpoch 00031: saving model to checkpoints/weights.31.hdf5\n1102/1102 [==============================] - 0s 73us/sample - loss: 0.2849 - accuracy: 0.8893\nEpoch 32/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3672 - accuracy: 0.9062\nEpoch 00032: saving model to checkpoints/weights.32.hdf5\n 736/1102 [===================>..........] - ETA: 0s - loss: 0.2750 - accuracy: 0.8981\nEpoch 00032: saving model to checkpoints/weights.32.hdf5\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.2830 - accuracy: 0.8911\nEpoch 33/100\n 896/1102 [=======================>......] - ETA: 0s - loss: 0.2819 - accuracy: 0.8862\nEpoch 00033: saving model to checkpoints/weights.33.hdf5\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.2816 - accuracy: 0.8884\nEpoch 34/100\n 896/1102 [=======================>......] - ETA: 0s - loss: 0.2822 - accuracy: 0.8962\nEpoch 00034: saving model to checkpoints/weights.34.hdf5\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.2803 - accuracy: 0.8929\nEpoch 35/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2915 - accuracy: 0.8750\nEpoch 00035: saving model to checkpoints/weights.35.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2772 - accuracy: 0.8920\nEpoch 36/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3411 - accuracy: 0.8438\nEpoch 00036: saving model to checkpoints/weights.36.hdf5\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.2749 - accuracy: 0.8975\nEpoch 37/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2301 - accuracy: 0.9688\nEpoch 00037: saving model to checkpoints/weights.37.hdf5\n1102/1102 [==============================] - 0s 77us/sample - loss: 0.2730 - accuracy: 0.8956\nEpoch 38/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2244 - accuracy: 0.9062\nEpoch 00038: saving model to checkpoints/weights.38.hdf5\n1102/1102 [==============================] - 0s 74us/sample - loss: 0.2728 - accuracy: 0.8947\nEpoch 39/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2920 - accuracy: 0.8750\nEpoch 00039: saving model to checkpoints/weights.39.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2726 - accuracy: 0.8911\nEpoch 40/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.4339 - accuracy: 0.8125\nEpoch 00040: saving model to checkpoints/weights.40.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2698 - accuracy: 0.8929\nEpoch 41/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2772 - accuracy: 0.9062\nEpoch 00041: saving model to checkpoints/weights.41.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2682 - accuracy: 0.9002\nEpoch 42/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3056 - accuracy: 0.8438\nEpoch 00042: saving model to checkpoints/weights.42.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2699 - accuracy: 0.8929\nEpoch 43/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1945 - accuracy: 0.9375\nEpoch 00043: saving model to checkpoints/weights.43.hdf5\n 640/1102 [================>.............] - ETA: 0s - loss: 0.2686 - accuracy: 0.8828\nEpoch 00043: saving model to checkpoints/weights.43.hdf5\n1102/1102 [==============================] - 0s 84us/sample - loss: 0.2695 - accuracy: 0.8929\nEpoch 44/100\n 800/1102 [====================>.........] - ETA: 0s - loss: 0.2761 - accuracy: 0.8938\nEpoch 00044: saving model to checkpoints/weights.44.hdf5\n1102/1102 [==============================] - 0s 77us/sample - loss: 0.2661 - accuracy: 0.8966\nEpoch 45/100\n 800/1102 [====================>.........] - ETA: 0s - loss: 0.2756 - accuracy: 0.8988\nEpoch 00045: saving model to checkpoints/weights.45.hdf5\n1102/1102 [==============================] - 0s 79us/sample - loss: 0.2657 - accuracy: 0.9011\nEpoch 46/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1171 - accuracy: 1.0000\nEpoch 00046: saving model to checkpoints/weights.46.hdf5\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.2629 - accuracy: 0.8993\nEpoch 47/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1158 - accuracy: 1.0000\nEpoch 00047: saving model to checkpoints/weights.47.hdf5\n1102/1102 [==============================] - 0s 74us/sample - loss: 0.2619 - accuracy: 0.8993\nEpoch 48/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1766 - accuracy: 0.9062\nEpoch 00048: saving model to checkpoints/weights.48.hdf5\n1102/1102 [==============================] - 0s 77us/sample - loss: 0.2605 - accuracy: 0.9011\nEpoch 49/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3393 - accuracy: 0.8750\nEpoch 00049: saving model to checkpoints/weights.49.hdf5\n1102/1102 [==============================] - 0s 78us/sample - loss: 0.2604 - accuracy: 0.9020\nEpoch 50/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.5448 - accuracy: 0.8125\nEpoch 00050: saving model to checkpoints/weights.50.hdf5\n1102/1102 [==============================] - 0s 79us/sample - loss: 0.2588 - accuracy: 0.8984\nEpoch 51/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1207 - accuracy: 1.0000\nEpoch 00051: saving model to checkpoints/weights.51.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2584 - accuracy: 0.9011\nEpoch 52/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2166 - accuracy: 0.9062\nEpoch 00052: saving model to checkpoints/weights.52.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2571 - accuracy: 0.9011\nEpoch 53/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.4264 - accuracy: 0.8125\nEpoch 00053: saving model to checkpoints/weights.53.hdf5\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.2576 - accuracy: 0.8966\nEpoch 54/100\n\nEpoch 00054: saving model to checkpoints/weights.54.hdf5\n 896/1102 [=======================>......] - ETA: 0s - loss: 0.2392 - accuracy: 0.9051\nEpoch 00054: saving model to checkpoints/weights.54.hdf5\n1102/1102 [==============================] - 0s 75us/sample - loss: 0.2559 - accuracy: 0.9002\nEpoch 55/100\n 864/1102 [======================>.......] - ETA: 0s - loss: 0.2614 - accuracy: 0.8981\nEpoch 00055: saving model to checkpoints/weights.55.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2546 - accuracy: 0.9011\nEpoch 56/100\n 800/1102 [====================>.........] - ETA: 0s - loss: 0.2607 - accuracy: 0.8950\nEpoch 00056: saving model to checkpoints/weights.56.hdf5\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.2550 - accuracy: 0.8956\nEpoch 57/100\n 736/1102 [===================>..........] - ETA: 0s - loss: 0.2614 - accuracy: 0.8981\nEpoch 00057: saving model to checkpoints/weights.57.hdf5\n1102/1102 [==============================] - 0s 78us/sample - loss: 0.2544 - accuracy: 0.8975\nEpoch 58/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3637 - accuracy: 0.8750\nEpoch 00058: saving model to checkpoints/weights.58.hdf5\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.2529 - accuracy: 0.8993\nEpoch 59/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3252 - accuracy: 0.8125\nEpoch 00059: saving model to checkpoints/weights.59.hdf5\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.2523 - accuracy: 0.9002\nEpoch 60/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1308 - accuracy: 0.9688\nEpoch 00060: saving model to checkpoints/weights.60.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2509 - accuracy: 0.9002\nEpoch 61/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3030 - accuracy: 0.8125\nEpoch 00061: saving model to checkpoints/weights.61.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2498 - accuracy: 0.8993\nEpoch 62/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1434 - accuracy: 0.9375\nEpoch 00062: saving model to checkpoints/weights.62.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2490 - accuracy: 0.9020\nEpoch 63/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1048 - accuracy: 0.9688\nEpoch 00063: saving model to checkpoints/weights.63.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2497 - accuracy: 0.9002\nEpoch 64/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2051 - accuracy: 0.9062\nEpoch 00064: saving model to checkpoints/weights.64.hdf5\n 704/1102 [==================>...........] - ETA: 0s - loss: 0.2595 - accuracy: 0.9034\nEpoch 00064: saving model to checkpoints/weights.64.hdf5\n1102/1102 [==============================] - 0s 82us/sample - loss: 0.2477 - accuracy: 0.9038\nEpoch 65/100\n 832/1102 [=====================>........] - ETA: 0s - loss: 0.2527 - accuracy: 0.8978\nEpoch 00065: saving model to checkpoints/weights.65.hdf5\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.2471 - accuracy: 0.8993\nEpoch 66/100\n 800/1102 [====================>.........] - ETA: 0s - loss: 0.2486 - accuracy: 0.9050\nEpoch 00066: saving model to checkpoints/weights.66.hdf5\n1102/1102 [==============================] - 0s 77us/sample - loss: 0.2459 - accuracy: 0.9038\nEpoch 67/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2672 - accuracy: 0.8750\nEpoch 00067: saving model to checkpoints/weights.67.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2449 - accuracy: 0.9011\nEpoch 68/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2598 - accuracy: 0.9062\nEpoch 00068: saving model to checkpoints/weights.68.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2434 - accuracy: 0.9029\nEpoch 69/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3041 - accuracy: 0.8438\nEpoch 00069: saving model to checkpoints/weights.69.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2446 - accuracy: 0.9038\nEpoch 70/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1901 - accuracy: 0.9375\nEpoch 00070: saving model to checkpoints/weights.70.hdf5\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.2431 - accuracy: 0.9029\nEpoch 71/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2372 - accuracy: 0.8750\nEpoch 00071: saving model to checkpoints/weights.71.hdf5\n1102/1102 [==============================] - 0s 81us/sample - loss: 0.2423 - accuracy: 0.9038\nEpoch 72/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3440 - accuracy: 0.8438\nEpoch 00072: saving model to checkpoints/weights.72.hdf5\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.2427 - accuracy: 0.9029\nEpoch 73/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1917 - accuracy: 0.9688\nEpoch 00073: saving model to checkpoints/weights.73.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2412 - accuracy: 0.9065\nEpoch 74/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2186 - accuracy: 0.9062\nEpoch 00074: saving model to checkpoints/weights.74.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2400 - accuracy: 0.9038\nEpoch 75/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1671 - accuracy: 0.9375\nEpoch 00075: saving model to checkpoints/weights.75.hdf5\n 736/1102 [===================>..........] - ETA: 0s - loss: 0.2196 - accuracy: 0.9076\nEpoch 00075: saving model to checkpoints/weights.75.hdf5\n1102/1102 [==============================] - 0s 77us/sample - loss: 0.2423 - accuracy: 0.9020\nEpoch 76/100\n 864/1102 [======================>.......] - ETA: 0s - loss: 0.2394 - accuracy: 0.9132\nEpoch 00076: saving model to checkpoints/weights.76.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2379 - accuracy: 0.9065\nEpoch 77/100\n 832/1102 [=====================>........] - ETA: 0s - loss: 0.2406 - accuracy: 0.9062\nEpoch 00077: saving model to checkpoints/weights.77.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2372 - accuracy: 0.9056\nEpoch 78/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1732 - accuracy: 0.9062\nEpoch 00078: saving model to checkpoints/weights.78.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2360 - accuracy: 0.9074\nEpoch 79/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3573 - accuracy: 0.8750\nEpoch 00079: saving model to checkpoints/weights.79.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2367 - accuracy: 0.9038\nEpoch 80/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1735 - accuracy: 0.9688\nEpoch 00080: saving model to checkpoints/weights.80.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2358 - accuracy: 0.9065\nEpoch 81/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.0632 - accuracy: 1.0000\nEpoch 00081: saving model to checkpoints/weights.81.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2339 - accuracy: 0.9074\nEpoch 82/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2023 - accuracy: 0.9062\nEpoch 00082: saving model to checkpoints/weights.82.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2338 - accuracy: 0.9047\nEpoch 83/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2079 - accuracy: 0.9375\nEpoch 00083: saving model to checkpoints/weights.83.hdf5\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.2340 - accuracy: 0.9074\nEpoch 84/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1768 - accuracy: 0.9375\nEpoch 00084: saving model to checkpoints/weights.84.hdf5\n1102/1102 [==============================] - 0s 75us/sample - loss: 0.2335 - accuracy: 0.9074\nEpoch 85/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1238 - accuracy: 0.9688\nEpoch 00085: saving model to checkpoints/weights.85.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2309 - accuracy: 0.9102\nEpoch 86/100\n\nEpoch 00086: saving model to checkpoints/weights.86.hdf5\n 864/1102 [======================>.......] - ETA: 0s - loss: 0.2279 - accuracy: 0.9144\nEpoch 00086: saving model to checkpoints/weights.86.hdf5\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.2299 - accuracy: 0.9102\nEpoch 87/100\n 832/1102 [=====================>........] - ETA: 0s - loss: 0.2383 - accuracy: 0.9002\nEpoch 00087: saving model to checkpoints/weights.87.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2310 - accuracy: 0.9056\nEpoch 88/100\n 832/1102 [=====================>........] - ETA: 0s - loss: 0.2170 - accuracy: 0.9123\nEpoch 00088: saving model to checkpoints/weights.88.hdf5\n1102/1102 [==============================] - 0s 73us/sample - loss: 0.2296 - accuracy: 0.9093\nEpoch 89/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3325 - accuracy: 0.9062\nEpoch 00089: saving model to checkpoints/weights.89.hdf5\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.2281 - accuracy: 0.9056\nEpoch 90/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.4068 - accuracy: 0.8438\nEpoch 00090: saving model to checkpoints/weights.90.hdf5\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.2281 - accuracy: 0.9056\nEpoch 91/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3123 - accuracy: 0.8750\nEpoch 00091: saving model to checkpoints/weights.91.hdf5\n1102/1102 [==============================] - 0s 79us/sample - loss: 0.2264 - accuracy: 0.9074\nEpoch 92/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2472 - accuracy: 0.9375\nEpoch 00092: saving model to checkpoints/weights.92.hdf5\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.2261 - accuracy: 0.9056\nEpoch 93/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3107 - accuracy: 0.8750\nEpoch 00093: saving model to checkpoints/weights.93.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2248 - accuracy: 0.9111\nEpoch 94/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1170 - accuracy: 0.9688\nEpoch 00094: saving model to checkpoints/weights.94.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2240 - accuracy: 0.9056\nEpoch 95/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1900 - accuracy: 0.9375\nEpoch 00095: saving model to checkpoints/weights.95.hdf5\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.2234 - accuracy: 0.9083\nEpoch 96/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.2177 - accuracy: 0.8750\nEpoch 00096: saving model to checkpoints/weights.96.hdf5\n 768/1102 [===================>..........] - ETA: 0s - loss: 0.2145 - accuracy: 0.9089\nEpoch 00096: saving model to checkpoints/weights.96.hdf5\n1102/1102 [==============================] - 0s 77us/sample - loss: 0.2239 - accuracy: 0.9056\nEpoch 97/100\n 928/1102 [========================>.....] - ETA: 0s - loss: 0.2275 - accuracy: 0.9041\nEpoch 00097: saving model to checkpoints/weights.97.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2231 - accuracy: 0.9065\nEpoch 98/100\n 896/1102 [=======================>......] - ETA: 0s - loss: 0.2207 - accuracy: 0.9107\nEpoch 00098: saving model to checkpoints/weights.98.hdf5\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.2253 - accuracy: 0.9065\nEpoch 99/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.3430 - accuracy: 0.9062\nEpoch 00099: saving model to checkpoints/weights.99.hdf5\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.2218 - accuracy: 0.9065\nEpoch 100/100\n  32/1102 [..............................] - ETA: 0s - loss: 0.1117 - accuracy: 0.9688\nEpoch 00100: saving model to checkpoints/weights.100.hdf5\n1102/1102 [==============================] - 0s 67us/sample - loss: 0.2204 - accuracy: 0.9047\n368/1 - 0s - loss: 0.1911 - accuracy: 0.8995\nLoss: 0.32228841535423114, Accuracy: 0.8994565010070801\n"
    }
   ],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq=1000)\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100,callbacks=[cp_callback])\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'checkpoints/weights.100.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-56da3a5a1496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Restore the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mnn_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpoints/weights.100.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Evaluate the model using the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    179\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    180\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1169\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   1170\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonData/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'checkpoints/weights.100.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 =  8\n",
    "hidden_nodes_layer2 = 5\n",
    "\n",
    "nn_new = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "nn_new.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Restore the model weights\n",
    "nn_new.load_weights(\"checkpoints/weights.100.hdf5\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " - accuracy: 0.8276\nEpoch 116/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 117/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 118/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 119/300\n1102/1102 [==============================] - 0s 55us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 120/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 121/300\n1102/1102 [==============================] - 0s 51us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 122/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 123/300\n1102/1102 [==============================] - 0s 53us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 124/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 125/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 126/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 127/300\n1102/1102 [==============================] - 0s 53us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 128/300\n1102/1102 [==============================] - 0s 55us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 129/300\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 130/300\n1102/1102 [==============================] - 0s 86us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 131/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 132/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 133/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 134/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 135/300\n1102/1102 [==============================] - 0s 55us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 136/300\n1102/1102 [==============================] - 0s 50us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 137/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 138/300\n1102/1102 [==============================] - 0s 48us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 139/300\n1102/1102 [==============================] - 0s 49us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 140/300\n1102/1102 [==============================] - 0s 47us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 141/300\n1102/1102 [==============================] - 0s 47us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 142/300\n1102/1102 [==============================] - 0s 51us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 143/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 144/300\n1102/1102 [==============================] - 0s 52us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 145/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 146/300\n1102/1102 [==============================] - 0s 84us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 147/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 148/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 149/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 150/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 151/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 152/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 153/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 154/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 155/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 156/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 157/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 158/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 159/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 160/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 161/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 162/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 163/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 164/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 165/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 166/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 167/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 168/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 169/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 170/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 171/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 172/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 173/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 174/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 175/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 176/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 177/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 178/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 179/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 180/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 181/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 182/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 183/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 184/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 185/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 186/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 187/300\n1102/1102 [==============================] - 0s 73us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 188/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 189/300\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 190/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 191/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 192/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 193/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 194/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 195/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 196/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 197/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 198/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 199/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 200/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 201/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 202/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 203/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 204/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 205/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 206/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 207/300\n1102/1102 [==============================] - 0s 55us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 208/300\n1102/1102 [==============================] - 0s 53us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 209/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 210/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 211/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 212/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 213/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 214/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 215/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 216/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 217/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 218/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 219/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 220/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 221/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 222/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 223/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 224/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 225/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 226/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 227/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 228/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 229/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 230/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 231/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 232/300\n1102/1102 [==============================] - 0s 55us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 233/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 234/300\n1102/1102 [==============================] - 0s 55us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 235/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 236/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 237/300\n1102/1102 [==============================] - 0s 53us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 238/300\n1102/1102 [==============================] - 0s 53us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 239/300\n1102/1102 [==============================] - 0s 50us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 240/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 241/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 242/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 243/300\n1102/1102 [==============================] - 0s 52us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 244/300\n1102/1102 [==============================] - 0s 50us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 245/300\n1102/1102 [==============================] - 0s 53us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 246/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 247/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 248/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 249/300\n1102/1102 [==============================] - 0s 51us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 250/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 251/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 252/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 253/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 254/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 255/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 256/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 257/300\n1102/1102 [==============================] - 0s 55us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 258/300\n1102/1102 [==============================] - 0s 51us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 259/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 260/300\n1102/1102 [==============================] - 0s 52us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 261/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 262/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 263/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 264/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 265/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 266/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 267/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 268/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 269/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 270/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 271/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 272/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 273/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 274/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 275/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 276/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 277/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 278/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 279/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 280/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 281/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 282/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 283/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 284/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 285/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 286/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 287/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 288/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 289/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 290/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 291/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 292/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 293/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 294/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 295/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 296/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 297/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 298/300\n1102/1102 [==============================] - 0s 56us/sample - loss: 0.4596 - accuracy: 0.8276\nEpoch 299/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.4595 - accuracy: 0.8276\nEpoch 300/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 0.4595 - accuracy: 0.8276\n"
    }
   ],
   "source": [
    "#Train the Model\n",
    "fit_model = nn.fit(X_train,y_train,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "368/1 - 0s - loss: 0.3381 - accuracy: 0.8723\nloss: 0.3893813737060713, Accuracy: 0.8722826242446899\n"
    }
   ],
   "source": [
    "# Evaluate the model using test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_15 (Dense)             (None, 8)                 448       \n_________________________________________________________________\ndense_16 (Dense)             (None, 8)                 72        \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 529\nTrainable params: 529\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "## Skill drill: test different Models\n",
    "\n",
    "#Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 =  8\n",
    "hidden_nodes_layer2 = 8\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "- accuracy: 0.7740\nEpoch 116/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 2.5457 - accuracy: 0.7550\nEpoch 117/300\n1102/1102 [==============================] - 0s 74us/sample - loss: 1.5268 - accuracy: 0.7595\nEpoch 118/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.9016 - accuracy: 0.7804\nEpoch 119/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.7571 - accuracy: 0.7904\nEpoch 120/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 0.8048 - accuracy: 0.7931\nEpoch 121/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 4.1013 - accuracy: 0.7423\nEpoch 122/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 1.4202 - accuracy: 0.7759\nEpoch 123/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 1.5161 - accuracy: 0.7731\nEpoch 124/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 1.3492 - accuracy: 0.7677\nEpoch 125/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 1.4762 - accuracy: 0.7559\nEpoch 126/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 1.8576 - accuracy: 0.7750\nEpoch 127/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 1.6749 - accuracy: 0.7713\nEpoch 128/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 1.1680 - accuracy: 0.7686\nEpoch 129/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 2.0851 - accuracy: 0.7477\nEpoch 130/300\n1102/1102 [==============================] - 0s 53us/sample - loss: 1.0997 - accuracy: 0.7849\nEpoch 131/300\n1102/1102 [==============================] - 0s 54us/sample - loss: 2.6403 - accuracy: 0.7514\nEpoch 132/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 2.3281 - accuracy: 0.7604\nEpoch 133/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 1.5933 - accuracy: 0.7641\nEpoch 134/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 1.0841 - accuracy: 0.7976\nEpoch 135/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.8400 - accuracy: 0.7922\nEpoch 136/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 1.3500 - accuracy: 0.7722\nEpoch 137/300\n1102/1102 [==============================] - 0s 55us/sample - loss: 1.0738 - accuracy: 0.7731\nEpoch 138/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 1.2064 - accuracy: 0.7704\nEpoch 139/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 1.2642 - accuracy: 0.7831\nEpoch 140/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 1.2069 - accuracy: 0.7813\nEpoch 141/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 1.1769 - accuracy: 0.7804\nEpoch 142/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 2.0224 - accuracy: 0.7641\nEpoch 143/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 2.5898 - accuracy: 0.7387\nEpoch 144/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 2.1061 - accuracy: 0.7468\nEpoch 145/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.7937 - accuracy: 0.7868\nEpoch 146/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.9211 - accuracy: 0.7904\nEpoch 147/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.5043 - accuracy: 0.7668\nEpoch 148/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 1.1271 - accuracy: 0.7722\nEpoch 149/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 1.0242 - accuracy: 0.7813\nEpoch 150/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 1.8865 - accuracy: 0.7505\nEpoch 151/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.0852 - accuracy: 0.7768\nEpoch 152/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.7913 - accuracy: 0.7940\nEpoch 153/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 1.5022 - accuracy: 0.7550\nEpoch 154/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.7075 - accuracy: 0.8113\nEpoch 155/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 1.4269 - accuracy: 0.7713\nEpoch 156/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 1.2302 - accuracy: 0.7795\nEpoch 157/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 1.3957 - accuracy: 0.7786\nEpoch 158/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.0613 - accuracy: 0.8058\nEpoch 159/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 1.3209 - accuracy: 0.7659\nEpoch 160/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.3799 - accuracy: 0.7750\nEpoch 161/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 1.4961 - accuracy: 0.7822\nEpoch 162/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 1.6200 - accuracy: 0.7550\nEpoch 163/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.3578 - accuracy: 0.7722\nEpoch 164/300\n1102/1102 [==============================] - 0s 73us/sample - loss: 1.0054 - accuracy: 0.7922\nEpoch 165/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 1.3531 - accuracy: 0.7623\nEpoch 166/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 0.7075 - accuracy: 0.7913\nEpoch 167/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.7290 - accuracy: 0.8049\nEpoch 168/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.0927 - accuracy: 0.7668\nEpoch 169/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.1789 - accuracy: 0.7677\nEpoch 170/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.3650 - accuracy: 0.7740\nEpoch 171/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 3.8010 - accuracy: 0.7450\nEpoch 172/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 1.3300 - accuracy: 0.7650\nEpoch 173/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.5624 - accuracy: 0.8240\nEpoch 174/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.3382 - accuracy: 0.7568\nEpoch 175/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 1.2921 - accuracy: 0.7849\nEpoch 176/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 1.1823 - accuracy: 0.7677\nEpoch 177/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.0001 - accuracy: 0.7840\nEpoch 178/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.9469 - accuracy: 0.7895\nEpoch 179/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.9281 - accuracy: 0.7414\nEpoch 180/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 2.6757 - accuracy: 0.7595\nEpoch 181/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.3187 - accuracy: 0.7677\nEpoch 182/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 0.8739 - accuracy: 0.7967\nEpoch 183/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 2.1218 - accuracy: 0.7514\nEpoch 184/300\n1102/1102 [==============================] - 0s 78us/sample - loss: 1.2133 - accuracy: 0.7722\nEpoch 185/300\n1102/1102 [==============================] - 0s 75us/sample - loss: 2.3993 - accuracy: 0.7668\nEpoch 186/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 1.9580 - accuracy: 0.7468\nEpoch 187/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.2350 - accuracy: 0.7686\nEpoch 188/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.8971 - accuracy: 0.8040\nEpoch 189/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 1.1578 - accuracy: 0.7840\nEpoch 190/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.0645 - accuracy: 0.7940\nEpoch 191/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.9209 - accuracy: 0.8049\nEpoch 192/300\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.6100 - accuracy: 0.8113\nEpoch 193/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 1.1796 - accuracy: 0.7877\nEpoch 194/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 2.2182 - accuracy: 0.7731\nEpoch 195/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.8407 - accuracy: 0.7931\nEpoch 196/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.7442 - accuracy: 0.7940\nEpoch 197/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.5920 - accuracy: 0.8230\nEpoch 198/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.7392 - accuracy: 0.7976\nEpoch 199/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 1.7427 - accuracy: 0.7831\nEpoch 200/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 1.1358 - accuracy: 0.7695\nEpoch 201/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.5983 - accuracy: 0.7740\nEpoch 202/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.3007 - accuracy: 0.7695\nEpoch 203/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.8450 - accuracy: 0.7913\nEpoch 204/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 0.7786 - accuracy: 0.8049\nEpoch 205/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 0.5547 - accuracy: 0.8240\nEpoch 206/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 1.1057 - accuracy: 0.7804\nEpoch 207/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.7166 - accuracy: 0.8103\nEpoch 208/300\n1102/1102 [==============================] - 0s 57us/sample - loss: 0.6245 - accuracy: 0.8167\nEpoch 209/300\n1102/1102 [==============================] - 0s 58us/sample - loss: 0.9239 - accuracy: 0.8004\nEpoch 210/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 2.0614 - accuracy: 0.7759\nEpoch 211/300\n1102/1102 [==============================] - 0s 85us/sample - loss: 1.1766 - accuracy: 0.7768\nEpoch 212/300\n1102/1102 [==============================] - 0s 112us/sample - loss: 0.9971 - accuracy: 0.7858\nEpoch 213/300\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.7191 - accuracy: 0.8212\nEpoch 214/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.9230 - accuracy: 0.7958\nEpoch 215/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.1778 - accuracy: 0.7922\nEpoch 216/300\n1102/1102 [==============================] - 0s 59us/sample - loss: 0.5002 - accuracy: 0.8267\nEpoch 217/300\n1102/1102 [==============================] - 0s 60us/sample - loss: 0.6409 - accuracy: 0.8058\nEpoch 218/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 1.4078 - accuracy: 0.7868\nEpoch 219/300\n1102/1102 [==============================] - 0s 63us/sample - loss: 1.6598 - accuracy: 0.8022\nEpoch 220/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.6500 - accuracy: 0.8085\nEpoch 221/300\n1102/1102 [==============================] - 0s 61us/sample - loss: 0.7170 - accuracy: 0.8149\nEpoch 222/300\n1102/1102 [==============================] - 0s 62us/sample - loss: 1.7834 - accuracy: 0.7822\nEpoch 223/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 0.9316 - accuracy: 0.7958\nEpoch 224/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.6843 - accuracy: 0.7995\nEpoch 225/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 1.2612 - accuracy: 0.7523\nEpoch 226/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.6518 - accuracy: 0.8303\nEpoch 227/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.2280 - accuracy: 0.7813\nEpoch 228/300\n1102/1102 [==============================] - 0s 88us/sample - loss: 1.6806 - accuracy: 0.7731\nEpoch 229/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 1.1084 - accuracy: 0.7958\nEpoch 230/300\n1102/1102 [==============================] - 0s 76us/sample - loss: 1.7337 - accuracy: 0.7713\nEpoch 231/300\n1102/1102 [==============================] - 0s 75us/sample - loss: 1.1362 - accuracy: 0.7958\nEpoch 232/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.8359 - accuracy: 0.8013\nEpoch 233/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.8096 - accuracy: 0.8049\nEpoch 234/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 0.6337 - accuracy: 0.8212\nEpoch 235/300\n1102/1102 [==============================] - 0s 64us/sample - loss: 1.0300 - accuracy: 0.7831\nEpoch 236/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.8075 - accuracy: 0.8022\nEpoch 237/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 1.2740 - accuracy: 0.7877\nEpoch 238/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 1.2597 - accuracy: 0.7632\nEpoch 239/300\n1102/1102 [==============================] - 0s 65us/sample - loss: 1.3068 - accuracy: 0.7831\nEpoch 240/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.7651 - accuracy: 0.7813\nEpoch 241/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.5076 - accuracy: 0.8330\nEpoch 242/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 3.5978 - accuracy: 0.7423\nEpoch 243/300\n1102/1102 [==============================] - 0s 74us/sample - loss: 2.0600 - accuracy: 0.7459\nEpoch 244/300\n1102/1102 [==============================] - 0s 95us/sample - loss: 1.3623 - accuracy: 0.7967\nEpoch 245/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 1.1012 - accuracy: 0.7768\nEpoch 246/300\n1102/1102 [==============================] - 0s 75us/sample - loss: 2.3384 - accuracy: 0.7668\nEpoch 247/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.6882 - accuracy: 0.8230\nEpoch 248/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.7931 - accuracy: 0.8022\nEpoch 249/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.6925 - accuracy: 0.8103\nEpoch 250/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.8611 - accuracy: 0.7577\nEpoch 251/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.5979 - accuracy: 0.8240\nEpoch 252/300\n1102/1102 [==============================] - 0s 74us/sample - loss: 1.6591 - accuracy: 0.7895\nEpoch 253/300\n1102/1102 [==============================] - 0s 77us/sample - loss: 0.4536 - accuracy: 0.8439\nEpoch 254/300\n1102/1102 [==============================] - 0s 74us/sample - loss: 0.5019 - accuracy: 0.8303\nEpoch 255/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.5070 - accuracy: 0.8412\nEpoch 256/300\n1102/1102 [==============================] - 0s 66us/sample - loss: 0.5105 - accuracy: 0.8303\nEpoch 257/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.2357 - accuracy: 0.7967\nEpoch 258/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.0519 - accuracy: 0.7995\nEpoch 259/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.8540 - accuracy: 0.8004\nEpoch 260/300\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.4873 - accuracy: 0.8376\nEpoch 261/300\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.7188 - accuracy: 0.8122\nEpoch 262/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.1872 - accuracy: 0.7559\nEpoch 263/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.7621 - accuracy: 0.8022\nEpoch 264/300\n1102/1102 [==============================] - 0s 81us/sample - loss: 0.7866 - accuracy: 0.8149\nEpoch 265/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 0.7268 - accuracy: 0.8167\nEpoch 266/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.9989 - accuracy: 0.7677\nEpoch 267/300\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.5405 - accuracy: 0.8212\nEpoch 268/300\n1102/1102 [==============================] - 0s 76us/sample - loss: 0.6279 - accuracy: 0.8258\nEpoch 269/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 0.5933 - accuracy: 0.8185\nEpoch 270/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 0.4385 - accuracy: 0.8521\nEpoch 271/300\n1102/1102 [==============================] - 0s 75us/sample - loss: 0.7568 - accuracy: 0.8031\nEpoch 272/300\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.5383 - accuracy: 0.8285\nEpoch 273/300\n1102/1102 [==============================] - 0s 76us/sample - loss: 1.3968 - accuracy: 0.7668\nEpoch 274/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.8477 - accuracy: 0.8040\nEpoch 275/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.8026 - accuracy: 0.8158\nEpoch 276/300\n1102/1102 [==============================] - 0s 73us/sample - loss: 1.2964 - accuracy: 0.7849\nEpoch 277/300\n1102/1102 [==============================] - 0s 78us/sample - loss: 0.5485 - accuracy: 0.8230\nEpoch 278/300\n1102/1102 [==============================] - 0s 73us/sample - loss: 0.9909 - accuracy: 0.7922\nEpoch 279/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.9858 - accuracy: 0.7868\nEpoch 280/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.8685 - accuracy: 0.7759\nEpoch 281/300\n1102/1102 [==============================] - 0s 72us/sample - loss: 0.6705 - accuracy: 0.8094\nEpoch 282/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.9403 - accuracy: 0.7886\nEpoch 283/300\n1102/1102 [==============================] - 0s 73us/sample - loss: 1.3726 - accuracy: 0.7976\nEpoch 284/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 2.0926 - accuracy: 0.7768\nEpoch 285/300\n1102/1102 [==============================] - 0s 71us/sample - loss: 1.3554 - accuracy: 0.7958\nEpoch 286/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 1.2384 - accuracy: 0.7877\nEpoch 287/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.5029 - accuracy: 0.8312\nEpoch 288/300\n1102/1102 [==============================] - 0s 68us/sample - loss: 0.8713 - accuracy: 0.8058\nEpoch 289/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.9041 - accuracy: 0.8076\nEpoch 290/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 0.9326 - accuracy: 0.7831\nEpoch 291/300\n1102/1102 [==============================] - 0s 67us/sample - loss: 0.4979 - accuracy: 0.8312\nEpoch 292/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.6799 - accuracy: 0.8131\nEpoch 293/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.7326 - accuracy: 0.8103\nEpoch 294/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.5643 - accuracy: 0.8358\nEpoch 295/300\n1102/1102 [==============================] - 0s 69us/sample - loss: 0.7176 - accuracy: 0.8076\nEpoch 296/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 0.4244 - accuracy: 0.8457\nEpoch 297/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 1.4536 - accuracy: 0.7768\nEpoch 298/300\n1102/1102 [==============================] - 0s 70us/sample - loss: 1.1292 - accuracy: 0.7686\nEpoch 299/300\n1102/1102 [==============================] - 0s 77us/sample - loss: 0.5469 - accuracy: 0.8321\nEpoch 300/300\n1102/1102 [==============================] - 0s 72us/sample - loss: 1.6787 - accuracy: 0.7586\n"
    }
   ],
   "source": [
    "#Train the Model\n",
    "fit_model = nn.fit(X_train,y_train,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn_new.save(\"trained_attrition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model to a new object\n",
    "nn_imported = tf.keras.models.load_model('trained_attrition.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "368/1 - 0s - loss: 0.6454 - accuracy: 0.7147\nLoss: 0.6842236104218856, Accuracy: 0.7146739363670349\n"
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "368/1 - 0s - loss: 1.2686 - accuracy: 0.8723\nloss: 1.8359110148056694, Accuracy: 0.8722826242446899\n"
    }
   ],
   "source": [
    "# Evaluate the model using test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1602785512446",
   "display_name": "Python 3.7.7 64-bit ('PythonData': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}